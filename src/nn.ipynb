{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06593c8a",
   "metadata": {},
   "source": [
    "# Brest Cancer Detection with PyTorch NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9365740",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6b0e1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80317378",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4f4f229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c00efa",
   "metadata": {},
   "source": [
    "### Split and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "15a3435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf718aa",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "db1d352d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.0287957 ,  0.58190223,  3.08097762, ...,  2.04090688,\n",
       "         0.36131005,  0.69974369],\n",
       "       [-1.21647338,  2.1334791 , -1.22259114, ..., -0.97191616,\n",
       "        -0.91392327, -0.64044058],\n",
       "       [-0.94391997,  0.29744647, -0.89843484, ..., -0.4061406 ,\n",
       "        -0.48071004,  0.17129705],\n",
       "       ...,\n",
       "       [-0.37598145, -1.63497199, -0.42670865, ..., -0.46311211,\n",
       "        -0.22139226,  0.58697209],\n",
       "       [ 0.16627141,  0.97919994,  0.11590082, ...,  0.34994404,\n",
       "        -0.67596107, -0.84473696],\n",
       "       [-0.38454334, -0.42897361, -0.41137133, ..., -0.86039746,\n",
       "        -0.35410194, -0.81640786]], shape=(386, 30))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1c4c9a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31cb59",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "15981755",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9d72fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c7f81",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6b83744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12e625e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BCNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f94e65",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0563a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.7360, Val Loss: 0.6684, LR: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.6928, Val Loss: 0.5975, LR: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6232, Val Loss: 0.5637, LR: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5890, Val Loss: 0.5192, LR: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5781, Val Loss: 0.4925, LR: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.5556, Val Loss: 0.4724, LR: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4846, Val Loss: 0.4658, LR: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.5318, Val Loss: 0.4441, LR: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.4937, Val Loss: 0.4263, LR: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.4796, Val Loss: 0.4157, LR: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.4555, Val Loss: 0.3982, LR: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.4186, Val Loss: 0.3948, LR: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.3895, Val Loss: 0.4099, LR: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.4019, Val Loss: 0.4058, LR: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.3783, Val Loss: 0.3740, LR: 0.001000\n",
      "Epoch 16/100, Train Loss: 0.3461, Val Loss: 0.3642, LR: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.3472, Val Loss: 0.3330, LR: 0.001000\n",
      "Epoch 18/100, Train Loss: 0.3748, Val Loss: 0.3207, LR: 0.001000\n",
      "Epoch 19/100, Train Loss: 0.3640, Val Loss: 0.3092, LR: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.3084, Val Loss: 0.2983, LR: 0.001000\n",
      "Epoch 21/100, Train Loss: 0.3634, Val Loss: 0.2870, LR: 0.001000\n",
      "Epoch 22/100, Train Loss: 0.2821, Val Loss: 0.2772, LR: 0.001000\n",
      "Epoch 23/100, Train Loss: 0.3523, Val Loss: 0.2668, LR: 0.001000\n",
      "Epoch 24/100, Train Loss: 0.2665, Val Loss: 0.2513, LR: 0.001000\n",
      "Epoch 25/100, Train Loss: 0.2749, Val Loss: 0.2599, LR: 0.001000\n",
      "Epoch 26/100, Train Loss: 0.2655, Val Loss: 0.2508, LR: 0.001000\n",
      "Epoch 27/100, Train Loss: 0.4030, Val Loss: 0.2463, LR: 0.001000\n",
      "Epoch 28/100, Train Loss: 0.2919, Val Loss: 0.2260, LR: 0.001000\n",
      "Epoch 29/100, Train Loss: 0.3586, Val Loss: 0.2122, LR: 0.001000\n",
      "Epoch 30/100, Train Loss: 0.2300, Val Loss: 0.2197, LR: 0.001000\n",
      "Epoch 31/100, Train Loss: 0.2961, Val Loss: 0.2022, LR: 0.001000\n",
      "Epoch 32/100, Train Loss: 0.3060, Val Loss: 0.2029, LR: 0.001000\n",
      "Epoch 33/100, Train Loss: 0.3417, Val Loss: 0.1878, LR: 0.001000\n",
      "Epoch 34/100, Train Loss: 0.2826, Val Loss: 0.1835, LR: 0.001000\n",
      "Epoch 35/100, Train Loss: 0.2004, Val Loss: 0.2179, LR: 0.001000\n",
      "Epoch 36/100, Train Loss: 0.2844, Val Loss: 0.1999, LR: 0.001000\n",
      "Epoch 37/100, Train Loss: 0.2136, Val Loss: 0.2053, LR: 0.001000\n",
      "Epoch 38/100, Train Loss: 0.1989, Val Loss: 0.1919, LR: 0.001000\n",
      "Epoch 39/100, Train Loss: 0.2133, Val Loss: 0.1822, LR: 0.001000\n",
      "Epoch 40/100, Train Loss: 0.2070, Val Loss: 0.1824, LR: 0.001000\n",
      "Epoch 41/100, Train Loss: 0.2270, Val Loss: 0.1613, LR: 0.001000\n",
      "Epoch 42/100, Train Loss: 0.3188, Val Loss: 0.1649, LR: 0.001000\n",
      "Epoch 43/100, Train Loss: 0.2690, Val Loss: 0.1610, LR: 0.001000\n",
      "Epoch 44/100, Train Loss: 0.2518, Val Loss: 0.1668, LR: 0.001000\n",
      "Epoch 45/100, Train Loss: 0.2301, Val Loss: 0.1673, LR: 0.001000\n",
      "Epoch 46/100, Train Loss: 0.3127, Val Loss: 0.1882, LR: 0.001000\n",
      "Epoch 47/100, Train Loss: 0.1997, Val Loss: 0.1515, LR: 0.001000\n",
      "Epoch 48/100, Train Loss: 0.1616, Val Loss: 0.1567, LR: 0.001000\n",
      "Epoch 49/100, Train Loss: 0.1885, Val Loss: 0.1500, LR: 0.001000\n",
      "Epoch 50/100, Train Loss: 0.2272, Val Loss: 0.1449, LR: 0.001000\n",
      "Epoch 51/100, Train Loss: 0.2485, Val Loss: 0.1496, LR: 0.001000\n",
      "Epoch 52/100, Train Loss: 0.1770, Val Loss: 0.1339, LR: 0.001000\n",
      "Epoch 53/100, Train Loss: 0.2299, Val Loss: 0.1238, LR: 0.001000\n",
      "Epoch 54/100, Train Loss: 0.2396, Val Loss: 0.1245, LR: 0.001000\n",
      "Epoch 55/100, Train Loss: 0.3256, Val Loss: 0.1277, LR: 0.001000\n",
      "Epoch 56/100, Train Loss: 0.2727, Val Loss: 0.1475, LR: 0.001000\n",
      "Epoch 57/100, Train Loss: 0.1478, Val Loss: 0.1257, LR: 0.001000\n",
      "Epoch 58/100, Train Loss: 0.2144, Val Loss: 0.1196, LR: 0.001000\n",
      "Epoch 59/100, Train Loss: 0.1975, Val Loss: 0.1240, LR: 0.001000\n",
      "Epoch 60/100, Train Loss: 0.1436, Val Loss: 0.1152, LR: 0.001000\n",
      "Epoch 61/100, Train Loss: 0.1520, Val Loss: 0.1190, LR: 0.001000\n",
      "Epoch 62/100, Train Loss: 0.1393, Val Loss: 0.1266, LR: 0.001000\n",
      "Epoch 63/100, Train Loss: 0.1566, Val Loss: 0.1251, LR: 0.001000\n",
      "Epoch 64/100, Train Loss: 0.2113, Val Loss: 0.1225, LR: 0.001000\n",
      "Epoch 65/100, Train Loss: 0.1417, Val Loss: 0.1137, LR: 0.001000\n",
      "Epoch 66/100, Train Loss: 0.2133, Val Loss: 0.1198, LR: 0.001000\n",
      "Epoch 67/100, Train Loss: 0.2969, Val Loss: 0.1038, LR: 0.001000\n",
      "Epoch 68/100, Train Loss: 0.2191, Val Loss: 0.1074, LR: 0.001000\n",
      "Epoch 69/100, Train Loss: 0.2725, Val Loss: 0.1180, LR: 0.001000\n",
      "Epoch 70/100, Train Loss: 0.1854, Val Loss: 0.1055, LR: 0.001000\n",
      "Epoch 71/100, Train Loss: 0.3802, Val Loss: 0.1093, LR: 0.001000\n",
      "Epoch 72/100, Train Loss: 0.2300, Val Loss: 0.1166, LR: 0.001000\n",
      "Epoch 73/100, Train Loss: 0.2782, Val Loss: 0.1250, LR: 0.000500\n",
      "Epoch 74/100, Train Loss: 0.1545, Val Loss: 0.1176, LR: 0.000500\n",
      "Epoch 75/100, Train Loss: 0.3157, Val Loss: 0.1147, LR: 0.000500\n",
      "Epoch 76/100, Train Loss: 0.2991, Val Loss: 0.1107, LR: 0.000500\n",
      "Epoch 77/100, Train Loss: 0.1451, Val Loss: 0.1148, LR: 0.000500\n",
      "Early stopping at epoch 77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100  # set high â€” early stopping will handle when to stop\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val_tensor)\n",
    "        val_loss = criterion(val_logits, y_val_tensor).item()\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad18104",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "12d89375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9824561476707458\n",
      "Loss:  0.11932429671287537\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(X_test_tensor)\n",
    "    loss = criterion(logits, y_test_tensor).item()\n",
    "    predictions = torch.sigmoid(logits)\n",
    "    accuracy = ((predictions >= 0.5) == y_test_tensor).float().mean().item()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Loss: \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
